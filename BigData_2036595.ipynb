{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQWYxnsMqQv"
      },
      "source": [
        "# **PySpark + Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFudfLCRNvXT"
      },
      "source": [
        "##  Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l6vtwpPGKn9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d326aa4d-e834-40f1-b006-bb68d1b119a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 49.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=34cfd11f6639ad6f3d98213d0dcd94b2081442295e924773e39fc3871029d45c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrmY8FiOMwa"
      },
      "source": [
        "##  Import useful PySpark packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fh8zPg5APmYv"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import requests\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbT2rM-4tvnB"
      },
      "source": [
        "##  Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xXQOJijlEz3I"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\").set('spark.executor.memory', '4G').set('spark.driver.memory', '45G').set('spark.driver.maxResultSize', '10G')\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRvyl2xwCV6"
      },
      "source": [
        "##  Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4fqJ5f0JE3BL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "753dceb5-fc93-4797-d2db-9686be49e9d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f3029b77c50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://48f99df0b889:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qhTN342EEOYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babe5cd7-ae01-424a-a3ac-23c44f731ba5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.memory', '45G'),\n",
              " ('spark.driver.port', '45005'),\n",
              " ('spark.driver.host', '48f99df0b889'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.driver.maxResultSize', '10G'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.ui.port', '4050'),\n",
              " ('spark.app.startTime', '1654631720437'),\n",
              " ('spark.app.id', 'local-1654631722023'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.executor.memory', '4G'),\n",
              " ('spark.ui.showConsoleProgress', 'true')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevlrMcPw1ZI"
      },
      "source": [
        "# **Data acquisition from kaggle**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important!** In order to fetch the dataset from kaggle **the user needs to provide the API with his personal kaggle.json file** provided by by Kaggle. After the execution of the cell below there will appear a button to upload the file, after which the cell will finish it's execution."
      ],
      "metadata": {
        "id": "mBtlmxdr6brN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUxqodqWTQC"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knR8YeABYcnL"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d kritanjalijain/amazon-reviews "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4g9KV8R3ZVhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10a7f9f-df5c-408e-b75e-c441c34c771a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  amazon-reviews.zip\n",
            "  inflating: amazon_review_polarity_csv.tgz  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip amazon-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qKi5Hd60FFcX"
      },
      "outputs": [],
      "source": [
        "train_df = spark.read.load('train.csv', format=\"csv\", sep=\",\", inferSchema=\"true\")\n",
        "test_df = spark.read.load('test.csv', format=\"csv\", sep=\",\", inferSchema=\"true\")\n",
        "\n",
        "# We limit the train and test set in order for all the models to fit into 25gb of ram.\n",
        "train_df = train_df.limit(200000)\n",
        "test_df = test_df.limit(40000)      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTd8ep9x74H"
      },
      "source": [
        "# **Data preprocessing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_AhkEcMSCeM"
      },
      "source": [
        "## Change column names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wh8hHLtK-x2m"
      },
      "outputs": [],
      "source": [
        "# Spark DataFrame\n",
        "columns = ['label', 'title', 'text']\n",
        "train_df = train_df.toDF(*columns)\n",
        "test_df = test_df.toDF(*columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete \"title\" column\n",
        "\n"
      ],
      "metadata": {
        "id": "Nh58RfU1DSkJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UOfOvmi4REsI"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(\"title\")\n",
        "test_df = test_df.drop(\"title\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete any null values"
      ],
      "metadata": {
        "id": "mHDM52jDDhAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()"
      ],
      "metadata": {
        "id": "fPf78H5N7RVY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make label of binary form - good reviews are 0, bad reviews are 1"
      ],
      "metadata": {
        "id": "Zb-drdSgF3Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.replace(2, 0)\n",
        "test_df = test_df.replace(2, 0)"
      ],
      "metadata": {
        "id": "CrP2KZAUF7dT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text cleaning pipeline**\n",
        "\n",
        "As **preliminary** steps of any NLP task, at least the following pipeline must be executed first:\n",
        "\n",
        "- Text cleaning:\n",
        " - Case normalization (<code>lower</code>) -> convert all text to lower case;\n",
        " - Filter out _leading_ and _trailing_ whitespaces (<code>trim</code>);\n",
        " - Filter out punctuation symbols (<code>regexp_replace</code>);\n",
        " - Filter out any internal extra whitespace resulting from the step above (<code>regexp_replace</code> + <code>trim</code>).\n",
        "- Tokenization (<code>Tokenizer</code>): splitting raw text into a list of individual _tokens_ (i.e., words), typically using whitespace as delimiter \n",
        "- Stopwords removal (<code>StopWordsRemover</code>): removing so-called _stopwords_, namely words that do not contribute to the deeper meaning of the document like \"the\", \"a\", \"me\", etc.\n",
        "- Stemming (<code>SnowballStemmer</code>): reducing each word to its root or base. For example \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\"."
      ],
      "metadata": {
        "id": "1bzS9k2-FCTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JyRyYqeXGA4l"
      },
      "outputs": [],
      "source": [
        "def clean_text(df, column_name=\"text\"):\n",
        "    \n",
        "    from pyspark.sql.functions import udf, col, lower, trim, regexp_replace\n",
        "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
        "    print(\"# 1. Text Cleaning\\n\")\n",
        "    \n",
        "    print(\"1.a Case normalization:\")\n",
        "    lower_case_news_df = df.select(\"label\", lower(col(column_name)).alias(column_name))\n",
        "    lower_case_news_df.show(10)\n",
        "   \n",
        "    print(\"1.b Trimming:\")\n",
        "    trimmed_news_df = lower_case_news_df.select(\"label\", trim(col(column_name)).alias(column_name))\n",
        "    trimmed_news_df.show(10)\n",
        "    \n",
        "    print(\"1.c Filter out punctuation:\")\n",
        "    no_punct_news_df = trimmed_news_df.select(\"label\", (regexp_replace(col(column_name), \"[^a-zA-Z\\\\s]\", \"\")).alias(column_name))\n",
        "    no_punct_news_df.show(10)\n",
        "    \n",
        "    print(\"1.d Filter out extra whitespaces:\")\n",
        "    cleaned_news_df = no_punct_news_df.select(\"label\", trim(regexp_replace(col(column_name), \" +\", \" \")).alias(column_name))\n",
        "    cleaned_news_df.show(10)\n",
        "\n",
        "    print(\"# 2. Tokenization:\")\n",
        "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n",
        "    tokens_df = tokenizer.transform(cleaned_news_df)\n",
        "    tokens_df.show(10)\n",
        "\n",
        "    print(\"# 3. Stopwords removal:\")\n",
        "    stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"terms\")\n",
        "    terms_df = stopwords_remover.transform(tokens_df)\n",
        "    terms_df.show(10)\n",
        "\n",
        "    print(\"# 4. Stemming:\")\n",
        "    stemmer = SnowballStemmer(language=\"english\")\n",
        "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))\n",
        "    terms_stemmed_df.show(10)\n",
        "    \n",
        "    return terms_stemmed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0fHR_fcA_cX"
      },
      "outputs": [],
      "source": [
        "clean_train_df = clean_text(train_df)\n",
        "clean_train_df.cache()\n",
        "\n",
        "clean_test_df = clean_text(test_df)\n",
        "clean_test_df.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WC4RPQgyEsB"
      },
      "source": [
        "#**Feature Engineering**\n",
        "\n",
        "Machine learning techniques cannot work directly on text data; in fact, words must be first converted into some numerical representation which machine learning algorithms can make use of. This process is often known as _embedding_ or _vectorization_.\n",
        "\n",
        "In terms of vectorization, it is important to remember that it isn't merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Moreover, vectors derived from text data are usually high-dimensional. This is because each dimension of the feature space will correspond to a word, and the language in the documents may have thousands of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CoH6G5nB9hC"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "In information retrieval, **tf-idf** - short for term frequency-inverse document frequency - is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P3dC7383CJGy"
      },
      "outputs": [],
      "source": [
        "def extract_tfidf_features(df, column_name=\"terms_stemmed\"):\n",
        "\n",
        "    from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
        "    from pyspark.ml import Pipeline\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=10000)\n",
        "    idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[hashingTF, idf])\n",
        "    features = pipeline.fit(df)\n",
        "    \n",
        "    tf_idf_features_df = features.transform(df)\n",
        "\n",
        "    return tf_idf_features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4B76emfeBiXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2747f36-71c2-44a6-d7d4-ceb627a310f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[label: int, text: string, tokens: array<string>, terms: array<string>, terms_stemmed: array<string>, tf_features: vector, features: vector]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tf_idf_train_df = extract_tfidf_features(clean_train_df)\n",
        "tf_idf_train_df.cache()\n",
        "\n",
        "tf_idf_test_df = extract_tfidf_features(clean_test_df)\n",
        "tf_idf_test_df.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-by1hDer9_a"
      },
      "source": [
        "### Clean-up unused variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2_nagGpWnAKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe486d6c-9181-4dd4-cf54-29291128e2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Garbage collector: collected 698 objects\n"
          ]
        }
      ],
      "source": [
        "# Delete objects which are not needed, and invoking python's garbage collector\n",
        "import gc\n",
        "del(train_df)\n",
        "del(test_df)\n",
        "del(clean_train_df)\n",
        "del(clean_test_df)\n",
        "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zKWMAFDcUo"
      },
      "source": [
        "### Check and remove any possible zero-length vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q1Rt7iQ6Dsg-"
      },
      "outputs": [],
      "source": [
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnSS_NnaE3CN"
      },
      "source": [
        "### Check if there is any zero-lenght vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0JNeCynsFBPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b8b1cd-dda8-4f5a-e0c5-97e50f15e886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total n. of zero-length vectors in train df: 12 and in test df: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"Total n. of zero-length vectors in train df: {:d}\".\n",
        "      format(tf_idf_train_df.where(num_nonzeros(\"features\") == 0).count()), \n",
        "      \"and in test df: {:d}\".\n",
        "      format(tf_idf_test_df.where(num_nonzeros(\"features\") == 0).count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0o-9OfWO4Dv"
      },
      "source": [
        "### Remove zero-lenght vector(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XYstFwh-CNhn"
      },
      "outputs": [],
      "source": [
        "tf_idf_train_df = tf_idf_train_df.where(num_nonzeros(\"features\") > 0)\n",
        "tf_idf_test_df = tf_idf_test_df.where(num_nonzeros(\"features\") > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Dfz_QoO-bH"
      },
      "source": [
        "### Double-check there is no more zero-length vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wHDwtiRXRSBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46963dbd-d0e2-4681-afbc-4416cadf4386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total n. of zero-length vectors (after removal): 0 and in test df: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Total n. of zero-length vectors (after removal): {:d}\".\n",
        "      format(tf_idf_train_df.where(num_nonzeros(\"features\") == 0).count()), \n",
        "      \"and in test df: {:d}\".\n",
        "      format(tf_idf_test_df.where(num_nonzeros(\"features\") == 0).count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oJ4nmOfcPG0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69c2a93-f94a-4331-9ea7-7a614a125138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Garbage collector: collected 384 objects\n"
          ]
        }
      ],
      "source": [
        "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select specific columns"
      ],
      "metadata": {
        "id": "9RpxGzVArSy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_train_df = tf_idf_train_df.select([\"features\", \"label\"])"
      ],
      "metadata": {
        "id": "1UG0uGGnrW3Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model training with different hyperparameter configurations and $k$-fold cross validation**\n",
        "\n",
        "In the following sections, we try we implement a pipeline making use of different hyperparameters and also of $k$-fold cross validation to get a better estimate of the generalization performance of our 3 models.\n",
        "1. Naive Bayes model\n",
        "2. Logistic Regression model\n",
        "3. Random Forest model\n"
      ],
      "metadata": {
        "id": "Cgc7ESMIrKqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Naive Bayes Classifier**\n",
        "Firstly, we will train a naive bayes classifier, using the training set above. \n",
        "\n",
        "We will use the `NaiveBayes` object provided by the [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier) within the package `pyspark.ml.classification`."
      ],
      "metadata": {
        "id": "pWSqsvQ0fue8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_pipeline(  train, \n",
        "                           with_std=True,\n",
        "                           with_mean=True,\n",
        "                           k_fold=5):\n",
        "\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    from pyspark.ml.classification import NaiveBayes\n",
        "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "    from pyspark.ml import Pipeline\n",
        " \n",
        "    stages = []\n",
        "    nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\") \n",
        "    stages += [nb]\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    # With 6 values for alpha (the smoothing parameter) which helps tackle the problem of zero probability\n",
        "    param_grid = ParamGridBuilder()\\\n",
        "    .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\\\n",
        "    .build()\n",
        "    \n",
        "    cross_val = CrossValidator(estimator=pipeline, \n",
        "                               estimatorParamMaps=param_grid,\n",
        "                               evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"), # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                               numFolds=k_fold,\n",
        "                               collectSubModels=True \n",
        "                               )\n",
        "    cv_model = cross_val.fit(train)\n",
        "\n",
        "    return cv_model"
      ],
      "metadata": {
        "id": "WsuBfIMKf6Gg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = naive_bayes_pipeline(tf_idf_train_df)"
      ],
      "metadata": {
        "id": "dnK8YDUKgw_5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, avg_roc_auc in enumerate(cv_model.avgMetrics):\n",
        "    print(\"Avg. ROC AUC computed across k-fold cross validation for model setting #{:d}: {:.3f}\".format(i+1, avg_roc_auc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZz7BhBxh5iQ",
        "outputId": "b6191b8c-330d-4781-85a5-abf2460417b5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ROC AUC computed across k-fold cross validation for model setting #1: 0.469\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #2: 0.469\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #3: 0.469\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #4: 0.469\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #5: 0.469\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #6: 0.469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best model according to k-fold cross validation, smoothing=[{:f}]\".\n",
        "      format(cv_model.bestModel.stages[-1].getSmoothing())\n",
        "      )\n",
        "print(cv_model.bestModel.stages[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whr9AguYh_3E",
        "outputId": "0a8a1d58-bbee-45df-82fe-cbe9f4e60f6f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model according to k-fold cross validation, smoothing=[0.000000]\n",
            "NaiveBayesModel: uid=NaiveBayes_9576143cb991, modelType=multinomial, numClasses=2, numFeatures=10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the best model from $k$-fold cross validation to make predictions"
      ],
      "metadata": {
        "id": "tX9qZ69KiekU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = cv_model.transform(tf_idf_test_df)"
      ],
      "metadata": {
        "id": "FV8Xpb-KieJu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions.select(\"features\", \"prediction\", \"label\").show(5)"
      ],
      "metadata": {
        "id": "TA8QR4hSinIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391aa1ff-bdf5-4864-8a0b-65d59b438b8b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----+\n",
            "|            features|prediction|label|\n",
            "+--------------------+----------+-----+\n",
            "|(10000,[281,306,3...|       0.0|    0|\n",
            "|(10000,[157,158,2...|       0.0|    0|\n",
            "|(10000,[157,355,4...|       1.0|    1|\n",
            "|(10000,[490,1604,...|       1.0|    0|\n",
            "|(10000,[15,157,15...|       0.0|    0|\n",
            "+--------------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance on the Test Set"
      ],
      "metadata": {
        "id": "Dsh2pGwGiqCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(predictions, metric=\"areaUnderROC\"):\n",
        "    \n",
        "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(metricName=metric)\n",
        "\n",
        "    return evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "wTOCollWOhdy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Test Set *****\")\n",
        "print(\"Area Under ROC Curve (ROC AUC): {:.3f}\".format(evaluate_model(test_predictions)))\n",
        "print(\"Area Under Precision-Recall Curve: {:.3f}\".format(evaluate_model(test_predictions, metric=\"areaUnderPR\")))\n",
        "print(\"***** Test Set *****\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEBOxI22iwC-",
        "outputId": "11f7b9e8-9926-4be8-d5a9-892b254ac2f6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Test Set *****\n",
            "Area Under ROC Curve (ROC AUC): 0.469\n",
            "Area Under Precision-Recall Curve: 0.464\n",
            "***** Test Set *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "Since we know that we have two classes to classify, a random predictor would have a theoretical maximum of $50\\%$ accuracy. For us to say that a model performed well enough, it should have a measurable advantage over the random predictor. This is not the case, as with the Naive Bayes Classifier, the model doen't seem to generalise well. "
      ],
      "metadata": {
        "id": "Fo191DoYI85U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2sZyYAp1tvF"
      },
      "source": [
        "## **2. Logistic Regression**\n",
        "\n",
        "We then train a logistic regression model, using the training set above. To do so, we use the `LogisticRegression` object provided by the [PySpark API](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression) within the package `pyspark.ml.classification`.\n",
        "\n",
        "\n",
        "More specifically, we will tune the two hyperparameters: \n",
        "- $\\lambda$ = `regParam` which is the regulation parameter\n",
        "- $\\alpha$ = `elasticNetParam` that is the tradeoff parameter for regularization penalties\n",
        "\n",
        "Specifically:\n",
        "  - `regParam = 0` and `elasticNetParam = 0` means there is no regularization;\n",
        "  - `regParam > 0` and `elasticNetParam = 0` means there is only L2-regularization; \n",
        "  - `regParam > 0` and `elasticNetParam = 1` means there is only L1-regularization;\n",
        "  - `regParam > 0` and `0 < elasticNetParam < 1` means there is both L1- and L2-regularization (Elastic Net);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_pipeline(train, \n",
        "                                 with_std=True,\n",
        "                                 with_mean=True,\n",
        "                                 k_fold=5):\n",
        "\n",
        "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    from pyspark.ml.classification import LogisticRegression\n",
        "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "    from pyspark.ml import Pipeline\n",
        "\n",
        "    stages = []\n",
        "    log_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
        "    stages += [log_reg]\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    # With 3 values for log_reg.regParam ($\\lambda$) and 3 values for log_reg.elasticNetParam ($\\alpha$),\n",
        "    # this grid will have 3 x 3 = 9 parameter settings for CrossValidator to choose from.\n",
        "    param_grid = ParamGridBuilder()\\\n",
        "    .addGrid(log_reg.regParam, [0.0, 0.05, 0.1]) \\\n",
        "    .addGrid(log_reg.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "    \n",
        "    cross_val = CrossValidator(estimator=pipeline, \n",
        "                               estimatorParamMaps=param_grid,\n",
        "                               evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"), # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                               numFolds=k_fold,\n",
        "                               collectSubModels=True \n",
        "                               )\n",
        "    cv_model = cross_val.fit(train)\n",
        "\n",
        "    return cv_model"
      ],
      "metadata": {
        "id": "OuSCTvFOMiBd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = logistic_regression_pipeline(tf_idf_train_df)"
      ],
      "metadata": {
        "id": "XZ_h6aboOU7C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function summarizes all the models trained during k-fold cross validation\n",
        "def summarize_all_models(cv_models):\n",
        "    for k, models in enumerate(cv_models):\n",
        "        print(\"*************** Fold #{:d} ***************\\n\".format(k+1))\n",
        "        for i, m in enumerate(models):\n",
        "            print(\"--- Model #{:d} out of {:d} ---\".format(i+1, len(models)))\n",
        "            print(\"\\tParameters: lambda=[{:.3f}]; alpha=[{:.3f}] \".format(m.stages[-1]._java_obj.getRegParam(), m.stages[-1]._java_obj.getElasticNetParam()))\n",
        "            print(\"\\tModel summary: {}\\n\".format(m.stages[-1]))\n",
        "        print(\"***************************************\\n\")"
      ],
      "metadata": {
        "id": "RLEK660UNEqI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_all_models(cv_model.subModels)"
      ],
      "metadata": {
        "id": "IUAeAHqRN8GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2835d2-c63e-41ee-a936-9f90eaaccaee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** Fold #1 ***************\n",
            "\n",
            "--- Model #1 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #2 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #3 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #4 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #5 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #6 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #7 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #8 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #9 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "***************************************\n",
            "\n",
            "*************** Fold #2 ***************\n",
            "\n",
            "--- Model #1 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #2 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #3 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #4 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #5 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #6 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #7 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #8 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #9 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "***************************************\n",
            "\n",
            "*************** Fold #3 ***************\n",
            "\n",
            "--- Model #1 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #2 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #3 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #4 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #5 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #6 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #7 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #8 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #9 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "***************************************\n",
            "\n",
            "*************** Fold #4 ***************\n",
            "\n",
            "--- Model #1 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #2 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #3 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #4 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #5 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #6 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #7 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #8 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #9 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "***************************************\n",
            "\n",
            "*************** Fold #5 ***************\n",
            "\n",
            "--- Model #1 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #2 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #3 out of 9 ---\n",
            "\tParameters: lambda=[0.000]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #4 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #5 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #6 out of 9 ---\n",
            "\tParameters: lambda=[0.050]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #7 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #8 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[0.500] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "--- Model #9 out of 9 ---\n",
            "\tParameters: lambda=[0.100]; alpha=[1.000] \n",
            "\tModel summary: LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n",
            "\n",
            "***************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, avg_roc_auc in enumerate(cv_model.avgMetrics):\n",
        "    print(\"Avg. ROC AUC computed across k-fold cross validation for model setting #{:d}: {:.3f}\".format(i+1, avg_roc_auc))"
      ],
      "metadata": {
        "id": "HMa3EcCPN-Dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd946a8-12c1-48b4-f690-16511e61ef46"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ROC AUC computed across k-fold cross validation for model setting #1: 0.894\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #2: 0.894\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #3: 0.894\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #4: 0.901\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #5: 0.809\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #6: 0.728\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #7: 0.903\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #8: 0.728\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #9: 0.500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best model according to k-fold cross validation: lambda=[{:.3f}]; alfa=[{:.3f}]\".\n",
        "      format(cv_model.bestModel.stages[-1]._java_obj.getRegParam(), \n",
        "             cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam(),\n",
        "             )\n",
        "      )\n",
        "print(cv_model.bestModel.stages[-1])"
      ],
      "metadata": {
        "id": "HXbjgOX2OAVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdf2cb8-22e6-43e8-e881-c42d26954cbe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model according to k-fold cross validation: lambda=[0.100]; alfa=[0.000]\n",
            "LogisticRegressionModel: uid=LogisticRegression_8b776098f592, numClasses=2, numFeatures=10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarize model performance on the Training Set"
      ],
      "metadata": {
        "id": "Z_O5DFoZOQ7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_result = cv_model.bestModel.stages[-1].summary\n",
        "print(\"***** Training Set *****\")\n",
        "print(\"Area Under ROC Curve (ROC AUC): {:.3f}\".format(training_result.areaUnderROC))\n",
        "print(\"Accuracy :\",training_result.accuracy)\n",
        "print(\"Recall:\",training_result.recallByLabel)\n",
        "print(\"Precision:\",training_result.precisionByLabel)\n",
        "print(\"***** Training Set *****\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_j8C9YAOWUW",
        "outputId": "3af743a2-812c-4cd8-f103-38dca7c20f2e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Training Set *****\n",
            "Area Under ROC Curve (ROC AUC): 0.924\n",
            "Accuracy : 0.8461657699461967\n",
            "Recall: [0.8674067557014206, 0.8244240050997177]\n",
            "Precision: [0.8348969533197587, 0.8586468542522921]\n",
            "***** Training Set *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the best model from $k$-fold cross validation to make predictions"
      ],
      "metadata": {
        "id": "H8qFiQzOOYsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = cv_model.transform(tf_idf_test_df)"
      ],
      "metadata": {
        "id": "z4GPSEqNObvz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions.select(\"features\", \"prediction\", \"label\").show(5)"
      ],
      "metadata": {
        "id": "TaAq1G7_OfKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27d5366-1772-411e-b737-6e85a4d799e6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----+\n",
            "|            features|prediction|label|\n",
            "+--------------------+----------+-----+\n",
            "|(10000,[281,306,3...|       0.0|    0|\n",
            "|(10000,[157,158,2...|       0.0|    0|\n",
            "|(10000,[157,355,4...|       1.0|    1|\n",
            "|(10000,[490,1604,...|       1.0|    0|\n",
            "|(10000,[15,157,15...|       0.0|    0|\n",
            "+--------------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance on the Test Set"
      ],
      "metadata": {
        "id": "5VIRg3s4PIwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Test Set *****\")\n",
        "print(\"Area Under ROC Curve (ROC AUC): {:.3f}\".format(evaluate_model(test_predictions)))\n",
        "print(\"Area Under Precision-Recall Curve: {:.3f}\".format(evaluate_model(test_predictions, metric=\"areaUnderPR\")))\n",
        "print(\"***** Test Set *****\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9XDbn49PJuB",
        "outputId": "368bbd07-8832-4d00-ca19-3057bc0aa3b8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Test Set *****\n",
            "Area Under ROC Curve (ROC AUC): 0.901\n",
            "Area Under Precision-Recall Curve: 0.898\n",
            "***** Test Set *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "This time we can see that this model in about the same training time performs much better than the Naive Bayes Classifier without the space complexity that is required by NB. For a non deep learning method, the results achieved across both metrics are very good."
      ],
      "metadata": {
        "id": "JIsMCNvBkAg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Random Forests**\n",
        "We now train a random forest, using the training set above.\n",
        "\n",
        "We will use the `RandomForestClassifier` object provided by the [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier) within the package `pyspark.ml.classification`."
      ],
      "metadata": {
        "id": "m9NHXdN5TNA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_pipeline(train, \n",
        "                           with_std=True,\n",
        "                           with_mean=True,\n",
        "                           k_fold=5):\n",
        "\n",
        "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "    from pyspark.ml.classification import RandomForestClassifier\n",
        "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "    from pyspark.ml import Pipeline\n",
        " \n",
        "    stages = []\n",
        "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "    stages += [rf]\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    # With 2 values for rf.maxDepth and 2 values for rf.numTrees\n",
        "    # this grid will have 2 x 2 = 4 parameter settings for CrossValidator to choose from.8,100\n",
        "    param_grid = ParamGridBuilder()\\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.numTrees, [10, 50]) \\\n",
        "    .build()\n",
        "    \n",
        "    cross_val = CrossValidator(estimator=pipeline, \n",
        "                               estimatorParamMaps=param_grid,\n",
        "                               evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"), # default = \"areaUnderROC\", alternatively \"areaUnderPR\"\n",
        "                               numFolds=k_fold,\n",
        "                               collectSubModels=True \n",
        "                               )\n",
        "    cv_model = cross_val.fit(train)\n",
        "    return cv_model"
      ],
      "metadata": {
        "id": "enmDkfV8TPbV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = random_forest_pipeline(tf_idf_train_df)"
      ],
      "metadata": {
        "id": "VDh4xB6iTaRy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, avg_roc_auc in enumerate(cv_model.avgMetrics):\n",
        "    print(\"Avg. ROC AUC computed across k-fold cross validation for model setting #{:d}: {:.3f}\".format(i+1, avg_roc_auc))"
      ],
      "metadata": {
        "id": "rSJDzSxiTg_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1fbbc2-b5dd-498d-c2c4-6c8f7bc4f4bc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ROC AUC computed across k-fold cross validation for model setting #1: 0.671\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #2: 0.804\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #3: 0.738\n",
            "Avg. ROC AUC computed across k-fold cross validation for model setting #4: 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best model according to k-fold cross validation: maxDept=[{:d}]\".\n",
        "      format(cv_model.bestModel.stages[-1]._java_obj.getNumTrees(), \n",
        "             )\n",
        "      )\n",
        "print(cv_model.bestModel.stages[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3GDrUHRTjCR",
        "outputId": "a9a0f550-8da6-450f-ea91-2bf87467bf7e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model according to k-fold cross validation: maxDept=[50]\n",
            "RandomForestClassificationModel: uid=RandomForestClassifier_f5c1d455d444, numTrees=50, numClasses=2, numFeatures=10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarize model performance on the Training Set"
      ],
      "metadata": {
        "id": "5zCv-KPyugaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_result = cv_model.bestModel.stages[-1].summary\n",
        "print(\"***** Training Set *****\")\n",
        "print(\"Area Under ROC Curve (ROC AUC): {:.3f}\".format(training_result.areaUnderROC))\n",
        "print(\"Recall:\",training_result.recallByLabel)\n",
        "print(\"Precision:\",training_result.precisionByLabel)\n",
        "print(\"Accuracy:\", training_result.accuracy)\n",
        "print(\"***** Training Set *****\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3mS8d_uR5C",
        "outputId": "390de2c5-0b98-4d4e-d800-f4b3340388e6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Training Set *****\n",
            "Area Under ROC Curve (ROC AUC): 0.821\n",
            "Recall: [0.8650540238634229, 0.5818332675631647]\n",
            "Precision: [0.679225365777933, 0.808145826599019]\n",
            "Accuracy: 0.7250935056103366\n",
            "***** Training Set *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the best model from $k$-fold cross validation to make predictions"
      ],
      "metadata": {
        "id": "l88nZfAmeuqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = cv_model.transform(tf_idf_test_df)"
      ],
      "metadata": {
        "id": "SRd3R8dheyGl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions.select(\"features\", \"prediction\", \"label\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3WSk8_Ve0Ua",
        "outputId": "ae856067-9c70-47aa-9b0f-ad1c77a29160"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----+\n",
            "|            features|prediction|label|\n",
            "+--------------------+----------+-----+\n",
            "|(10000,[281,306,3...|       0.0|    0|\n",
            "|(10000,[157,158,2...|       0.0|    0|\n",
            "|(10000,[157,355,4...|       1.0|    1|\n",
            "|(10000,[490,1604,...|       1.0|    0|\n",
            "|(10000,[15,157,15...|       1.0|    0|\n",
            "+--------------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance on the Test Set"
      ],
      "metadata": {
        "id": "Hn7DVeVUe5Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Test Set *****\")\n",
        "print(\"Area Under ROC Curve (ROC AUC): {:.3f}\".format(evaluate_model(test_predictions)))\n",
        "print(\"Area Under Precision-Recall Curve: {:.3f}\".format(evaluate_model(test_predictions, metric=\"areaUnderPR\")))\n",
        "print(\"***** Test Set *****\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mam7i6TifJym",
        "outputId": "b2ddd65c-9ee9-4d0c-ec6d-498eec3940c2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Test Set *****\n",
            "Area Under ROC Curve (ROC AUC): 0.817\n",
            "Area Under Precision-Recall Curve: 0.814\n",
            "***** Test Set *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "This time although the model performs better than the NB classifier, yielding measurable results, compared to the model which used Logistic Regression for Classification, it achieves worse metric results while it takes more time and more memory."
      ],
      "metadata": {
        "id": "2XiezEFOsfet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Remarks**\n",
        "As we observed, out of the 3 models, the model which used the Logistic Regression Classifier was able to outperform the other two. While not trading off many computational resources, it was able to achieve great results for a non DL method."
      ],
      "metadata": {
        "id": "VwAXexuZxuq8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BigData_2036595.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}